{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pycocotools\n",
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pycocotools.coco import COCO\n",
        "import csv\n",
        "\n",
        "import torch, os, time\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "from itertools import product\n",
        "import copy\n",
        "import os\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = Path('data/')\n",
        "src_json = data_dir / \"annotations.json\"              # original 60-class annotations\n",
        "dst_json = data_dir / \"annotations_10cats.json\"\n",
        "out_csv  = data_dir / \"image_labels.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Wrote 1500 rows to data/image_labels.csv\n"
          ]
        }
      ],
      "source": [
        "old_to_new = {\n",
        "    0:2, 1:8, 2:8, 3:8, 4:0, 5:0, 6:1, 7:0, 8:2, 9:1, 10:2, 11:2, 12:2,\n",
        "    13:3, 14:3, 15:3, 16:3, 17:3, 18:3, 19:3, 20:3, 21:0, 22:7, 23:1,\n",
        "    24:0, 25:5, 26:1, 27:0, 28:2, 29:4, 30:3, 31:3, 32:3, 33:3, 34:3,\n",
        "    35:3, 36:4, 37:0, 38:0, 39:0, 40:0, 41:0, 42:0, 43:0, 44:0, 45:0,\n",
        "    46:7, 47:0, 48:0, 49:0, 50:2, 51:6, 52:2, 53:6, 54:0, 55:0, 56:3,\n",
        "    57:7, 58:9, 59:9\n",
        "}\n",
        "\n",
        "grand_names = [\n",
        "    \"Plastic containers & bottles\",\n",
        "    \"Glass\",\n",
        "    \"Metal\",\n",
        "    \"Paper & cardboard\",\n",
        "    \"Plastic film / wrappers\",\n",
        "    \"Food waste / organics\",\n",
        "    \"Textiles & misc. items\",\n",
        "    \"Polystyrene / foam\",\n",
        "    \"Hazardous / special waste\",\n",
        "    \"Other / unlabelled\"\n",
        "]\n",
        "\n",
        "# ----- Load original TACO annotations -----\n",
        "with open(src_json) as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "# ----- Determine a single grand category per image -----\n",
        "image_to_cat = {}   # image_id -> grand_id\n",
        "for ann in coco[\"annotations\"]:\n",
        "    img_id = ann[\"image_id\"]\n",
        "    grand_id = old_to_new[ann[\"category_id\"]]\n",
        "    # policy: choose the lowest grand_id if multiple present\n",
        "    if img_id not in image_to_cat:\n",
        "        image_to_cat[img_id] = grand_id\n",
        "    else:\n",
        "        image_to_cat[img_id] = min(image_to_cat[img_id], grand_id)\n",
        "\n",
        "# ----- Build a simple table: image file name -> grand category -----\n",
        "rows = []\n",
        "for img in coco[\"images\"]:\n",
        "    img_id = img[\"id\"]\n",
        "    if img_id in image_to_cat:   # skip images with no annotations\n",
        "        rows.append({\n",
        "            \"file_name\": img[\"file_name\"],\n",
        "            \"grand_id\": image_to_cat[img_id],\n",
        "            \"grand_name\": grand_names[image_to_cat[img_id]]\n",
        "        })\n",
        "\n",
        "# ----- Save to CSV -----\n",
        "with open(out_csv, \"w\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"file_name\", \"grand_id\", \"grand_name\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(rows)\n",
        "\n",
        "print(f\"✓ Wrote {len(rows)} rows to {out_csv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 1200  Test: 300\n",
            "✓ DataLoaders ready for training\n"
          ]
        }
      ],
      "source": [
        "# %pip install Pillow\n",
        "# %pip install torch torchvision\n",
        "\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from PIL import Image\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torchvision import transforms\n",
        "# import torch\n",
        "\n",
        "# data_dir   = Path(\"data\")\n",
        "# images_dir = data_dir / \"images\"\n",
        "# csv_file   = data_dir / \"image_labels.csv\"\n",
        "\n",
        "# # ---------- Train / Test split ----------\n",
        "# df = pd.read_csv(csv_file)\n",
        "# train_df, test_df = train_test_split(\n",
        "#     df,\n",
        "#     test_size=0.2,        # 80% train, 20% test\n",
        "#     random_state=42,\n",
        "#     shuffle=True\n",
        "# )\n",
        "# print(f\"Train: {len(train_df)}  Test: {len(test_df)}\")\n",
        "\n",
        "# # (Optional) save the splits\n",
        "# train_df.to_csv(data_dir / \"train_labels.csv\", index=False)\n",
        "# test_df.to_csv(data_dir / \"test_labels.csv\", index=False)\n",
        "\n",
        "# # ---------- Transform: resize shortest side to 1024, keep full image ----------\n",
        "# train_transform = transforms.Compose([\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "#     transforms.Resize(1024),    # shortest side = 1024 px, keep aspect ratio\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                          std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# test_transform = transforms.Compose([\n",
        "#     transforms.Resize(1024),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                          std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "\n",
        "\n",
        "# # ---------- Custom Dataset ----------\n",
        "# class SingleLabelDataset(Dataset):\n",
        "#     def __init__(self, dataframe, img_root, transform=None):\n",
        "#         self.df = dataframe.reset_index(drop=True)\n",
        "#         self.root = Path(img_root)\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         row = self.df.iloc[idx]\n",
        "#         img = Image.open(self.root / row.file_name).convert(\"RGB\")\n",
        "#         label = torch.tensor(int(row.grand_id), dtype=torch.long)\n",
        "#         if self.transform:\n",
        "#             img = self.transform(img)\n",
        "#         return img, label\n",
        "\n",
        "# # ---------- Create Datasets & DataLoaders ----------\n",
        "# train_ds = SingleLabelDataset(train_df, images_dir, transform=train_transform)\n",
        "# test_ds  = SingleLabelDataset(test_df,  images_dir, transform=test_transform)\n",
        "\n",
        "# train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4)\n",
        "# test_loader  = DataLoader(test_ds,  batch_size=8, shuffle=False, num_workers=4)\n",
        "\n",
        "# print(\"✓ DataLoaders ready: 1024-pixel images without center crop.\")\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# ---------- Paths ----------\n",
        "data_dir   = Path(\"data\")\n",
        "images_dir = data_dir\n",
        "csv_file   = data_dir / \"image_labels.csv\"\n",
        "\n",
        "# ---------- Train / Test split ----------\n",
        "df = pd.read_csv(csv_file)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
        "print(f\"Train: {len(train_df)}  Test: {len(test_df)}\")\n",
        "\n",
        "# ---------- Transforms ----------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.Resize((512, 512)),     # shortest side = 1024, keep aspect ratio\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# ---------- Load all images once and wrap in TensorDataset ----------\n",
        "def make_tensor_dataset(frame, transform):\n",
        "    imgs, labels = [], []\n",
        "    for _, row in frame.iterrows():\n",
        "        img = Image.open(images_dir / row.file_name).convert(\"RGB\")\n",
        "        img = transform(img)\n",
        "        imgs.append(img)\n",
        "        labels.append(int(row.grand_id))\n",
        "    return TensorDataset(torch.stack(imgs),\n",
        "                         torch.tensor(labels, dtype=torch.long))\n",
        "\n",
        "train_ds = make_tensor_dataset(train_df, train_transform)\n",
        "test_ds  = make_tensor_dataset(test_df,  test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=8, shuffle=False)\n",
        "\n",
        "print(\"✓ DataLoaders ready for training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "\tdef __init__(self, in_channels: int, num_classes: int = 10):\n",
        "\t\t\tsuper(CNN, self).__init__()\n",
        "\t\t\t# ---- Convolutional feature extractor ----\n",
        "\t\t\tself.conv1 = nn.Conv2d(in_channels, 8, kernel_size=3, stride=1, padding=1)\n",
        "\t\t\tself.pool  = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\t\t\tself.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "\t\t\t# ---- Global pooling + classifier ----\n",
        "\t\t\t# Always outputs (batch, 16, 1, 1) no matter the input H×W\n",
        "\t\t\tself.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\t\t\tself.fc1 = nn.Linear(16, num_classes)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t\tx = F.relu(self.conv1(x))\n",
        "\t\t\tx = self.pool(x)\n",
        "\t\t\tx = F.relu(self.conv2(x))\n",
        "\t\t\tx = self.gap(x)                # -> (batch, 16, 1, 1)\n",
        "\t\t\tx = torch.flatten(x, 1)        # -> (batch, 16)\n",
        "\t\t\tx = self.fc1(x)                # logits\n",
        "\t\t\treturn x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")   # Use Apple Silicon GPU\n",
        "else:\n",
        "    device = torch.device(\"cpu\")   \n",
        "    \n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AES3iueY8Bja"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01: Train Loss 1.7521, Train Acc 48.67%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 02: Train Loss 1.5487, Train Acc 51.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 03: Train Loss 1.5365, Train Acc 51.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 04: Train Loss 1.5332, Train Acc 51.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 05: Train Loss 1.5308, Train Acc 51.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 06: Train Loss 1.5238, Train Acc 51.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 07: Train Loss 1.5232, Train Acc 51.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 08: Train Loss 1.5122, Train Acc 51.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 09: Train Loss 1.5128, Train Acc 51.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Train Loss 1.5096, Train Acc 51.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Train Loss 1.5067, Train Acc 51.58%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Train Loss 1.5019, Train Acc 51.67%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Train Loss 1.5033, Train Acc 51.67%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Train Loss 1.5019, Train Acc 51.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Train Loss 1.4987, Train Acc 51.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Train Loss 1.4964, Train Acc 51.58%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Train Loss 1.4968, Train Acc 51.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Train Loss 1.4993, Train Acc 51.67%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Train Loss 1.4935, Train Acc 51.42%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Train Loss 1.4941, Train Acc 52.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "model = CNN(in_channels=3, num_classes=10).to(device)\n",
        "\n",
        "# (Optional) add extra data‐augmentation transforms if needed:\n",
        "aug_tfms = transforms.Compose([\n",
        "\ttransforms.RandomHorizontalFlip(),\n",
        "\ttransforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    # wrap the train_loader with tqdm\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "\n",
        "    for imgs, labels in progress_bar:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update stats\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        # update the tqdm bar text\n",
        "        progress_bar.set_postfix({\n",
        "            \"loss\": f\"{running_loss/total:.4f}\",\n",
        "            \"acc\":  f\"{100*correct/total:.2f}%\"\n",
        "        })\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}: \"\n",
        "          f\"Train Loss {running_loss/total:.4f}, \"\n",
        "          f\"Train Acc {100*correct/total:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ukZOfaAlmAc"
      },
      "outputs": [],
      "source": [
        "DRIVE = Path(\"/content/drive/My Drive/dataset_splits\")\n",
        "DATA_ROOT = DRIVE / 'dataset_splits'\n",
        "\n",
        "TRAIN_DIR = DATA_ROOT / \"train\"\n",
        "TEST_DIR = DATA_ROOT / \"test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im6f6uno8FBI"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        # Layer 1: conv2D\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "        # Layer 2: 2x2 max pooling\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Layer 3: conv2D\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        # Layer 4: Fully connected layer\n",
        "        self.fc1 = nn.Linear(16 * 14 * 14, num_classes)  # 3136\n",
        "``\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUyGxN8p8ItV"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters that cannot be changed\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "# Hyperparameters that can be tuned\n",
        "learning_rate = 1e-2\n",
        "batch_size = 64\n",
        "num_epochs = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBDb8jAmrZYi"
      },
      "outputs": [],
      "source": [
        "# transforms\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojhuy2yW8LRq",
        "outputId": "59a7afc7-0b64-4753-82c3-8847ceba59d0"
      },
      "outputs": [],
      "source": [
        "train_dataset   = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
        "test_dataset = datasets.ImageFolder(TRAIN_DIR, transform=eval_tfms)\n",
        "\n",
        "# create validation set\n",
        "val_ratio = 0.15\n",
        "valLen = int(len(train_dataset) * val_ratio)\n",
        "trainLen = len(train_dataset) - valLen\n",
        "train_ds, val_ds = torch.utils.data.random_split(train_dataset, [trainLen, valLen])\n",
        "\n",
        "# loaders\n",
        "test_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(dataset=val_ds, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cQMpAWd18M7x",
        "outputId": "ad616c6c-16ae-481b-df9d-714627f8c0cf"
      },
      "outputs": [],
      "source": [
        "model = CNN(in_channels=1, num_classes=num_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tJ8_bu5H8PdW"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "5GFm-Jb18R18"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      n_samples = 0\n",
        "\n",
        "      for images, labels in train_loader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          # reset gradients\n",
        "          optimizer.zero_grad()\n",
        "          # backpropagate\n",
        "          loss.backward()\n",
        "          # update model weights\n",
        "          optimizer.step()\n",
        "\n",
        "          bs = images.size(0)\n",
        "          total_loss += loss.item() * bs\n",
        "          n_samples += bs\n",
        "\n",
        "      return total_loss / n_samples\n",
        "      # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {averageLoss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKPWkTB-De8B"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, device, criterion):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for images, labels in test_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      total_loss += loss.item() * images.size(0)\n",
        "      preds = outputs.argmax(1)\n",
        "      correct += (preds == labels).sum().item()\n",
        "      total += labels.size(0)\n",
        "\n",
        "  test_loss = total_loss / total\n",
        "  test_acc  = correct / total\n",
        "  return test_loss, test_acc\n",
        "  # print(f\"Test loss {test_loss:.4f} | Test acc {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHAACaFyw97O"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    \"optimizer\": [\"sgd\", \"adam\"],\n",
        "    \"lr\": [1e-1, 1e-2, 1e-3],\n",
        "    \"weight_decay\": [0.0, 1e-4, 1e-3],\n",
        "}\n",
        "search_epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmYVEXOLxGvf"
      },
      "outputs": [],
      "source": [
        "best = {\"val_acc\": -1.0, \"params\": None, \"state\": None}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCkSN7x7xHIm",
        "outputId": "ad215c9a-54ba-4ae6-e30c-ebb8c2137a75"
      },
      "outputs": [],
      "source": [
        "for opt_name, lr, wd in product(param_grid[\"optimizer\"], param_grid[\"lr\"], param_grid[\"weight_decay\"]):\n",
        "    model = CNN(in_channels=1, num_classes=10).to(device)\n",
        "\n",
        "    if opt_name == \"sgd\":\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "    for _ in range(search_epochs):\n",
        "        train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "    val_loss, val_acc = evaluate(model, val_loader, device, criterion)\n",
        "    print(f\"[{opt_name}] lr={lr:.0e}, wd={wd:.0e} -> val_acc={val_acc:.4f}, val_loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_acc > best[\"val_acc\"]:\n",
        "        best = {\"val_acc\": val_acc, \"params\": (opt_name, lr, wd), \"state\": copy.deepcopy(model.state_dict())}\n",
        "\n",
        "print(\"\\nBest params:\", best[\"params\"], \" | best val_acc:\", round(best[\"val_acc\"], 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV6wRm02zRGY"
      },
      "source": [
        "Try experimenting different numbers of layers in CNN model\n",
        "Ways:\n",
        "    - Fully connected\n",
        "    - More convolutional layers"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "acm_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
