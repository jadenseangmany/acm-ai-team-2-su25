{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pycocotools in /opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages (2.0.10)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages (from pycocotools) (2.3.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages (1.7.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages (from scikit-learn) (2.3.3)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pycocotools\n",
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pycocotools.coco import COCO\n",
        "import csv\n",
        "\n",
        "import torch, os, time\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "from itertools import product\n",
        "import copy\n",
        "import os\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = Path('data/')\n",
        "src_json = data_dir / \"annotations.json\"              # original 60-class annotations\n",
        "dst_json = data_dir / \"annotations_10cats.json\"\n",
        "out_csv  = data_dir / \"image_labels.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Wrote 1500 rows to data/image_labels.csv\n"
          ]
        }
      ],
      "source": [
        "old_to_new = {\n",
        "    0:2, 1:8, 2:8, 3:8, 4:0, 5:0, 6:1, 7:0, 8:2, 9:1, 10:2, 11:2, 12:2,\n",
        "    13:3, 14:3, 15:3, 16:3, 17:3, 18:3, 19:3, 20:3, 21:0, 22:7, 23:1,\n",
        "    24:0, 25:5, 26:1, 27:0, 28:2, 29:4, 30:3, 31:3, 32:3, 33:3, 34:3,\n",
        "    35:3, 36:4, 37:0, 38:0, 39:0, 40:0, 41:0, 42:0, 43:0, 44:0, 45:0,\n",
        "    46:7, 47:0, 48:0, 49:0, 50:2, 51:6, 52:2, 53:6, 54:0, 55:0, 56:3,\n",
        "    57:7, 58:9, 59:9\n",
        "}\n",
        "\n",
        "grand_names = [\n",
        "    \"Plastic containers & bottles\",\n",
        "    \"Glass\",\n",
        "    \"Metal\",\n",
        "    \"Paper & cardboard\",\n",
        "    \"Plastic film / wrappers\",\n",
        "    \"Food waste / organics\",\n",
        "    \"Textiles & misc. items\",\n",
        "    \"Polystyrene / foam\",\n",
        "    \"Hazardous / special waste\",\n",
        "    \"Other / unlabelled\"\n",
        "]\n",
        "\n",
        "# ----- Load original TACO annotations -----\n",
        "with open(src_json) as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "# ----- Determine a single grand category per image -----\n",
        "image_to_cat = {}   # image_id -> grand_id\n",
        "for ann in coco[\"annotations\"]:\n",
        "    img_id = ann[\"image_id\"]\n",
        "    grand_id = old_to_new[ann[\"category_id\"]]\n",
        "    # policy: choose the lowest grand_id if multiple present\n",
        "    if img_id not in image_to_cat:\n",
        "        image_to_cat[img_id] = grand_id\n",
        "    else:\n",
        "        image_to_cat[img_id] = min(image_to_cat[img_id], grand_id)\n",
        "\n",
        "# ----- Build a simple table: image file name -> grand category -----\n",
        "rows = []\n",
        "for img in coco[\"images\"]:\n",
        "    img_id = img[\"id\"]\n",
        "    if img_id in image_to_cat:   # skip images with no annotations\n",
        "        rows.append({\n",
        "            \"file_name\": img[\"file_name\"],\n",
        "            \"grand_id\": image_to_cat[img_id],\n",
        "            \"grand_name\": grand_names[image_to_cat[img_id]]\n",
        "        })\n",
        "\n",
        "# ----- Save to CSV -----\n",
        "with open(out_csv, \"w\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"file_name\", \"grand_id\", \"grand_name\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(rows)\n",
        "\n",
        "print(f\"✓ Wrote {len(rows)} rows to {out_csv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 1200  Test: 300\n",
            "✓ DataLoaders ready for training\n"
          ]
        }
      ],
      "source": [
        "# %pip install Pillow\n",
        "# %pip install torch torchvision\n",
        "\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from PIL import Image\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torchvision import transforms\n",
        "# import torch\n",
        "\n",
        "# data_dir   = Path(\"data\")\n",
        "# images_dir = data_dir / \"images\"\n",
        "# csv_file   = data_dir / \"image_labels.csv\"\n",
        "\n",
        "# # ---------- Train / Test split ----------\n",
        "# df = pd.read_csv(csv_file)\n",
        "# train_df, test_df = train_test_split(\n",
        "#     df,\n",
        "#     test_size=0.2,        # 80% train, 20% test\n",
        "#     random_state=42,\n",
        "#     shuffle=True\n",
        "# )\n",
        "# print(f\"Train: {len(train_df)}  Test: {len(test_df)}\")\n",
        "\n",
        "# # (Optional) save the splits\n",
        "# train_df.to_csv(data_dir / \"train_labels.csv\", index=False)\n",
        "# test_df.to_csv(data_dir / \"test_labels.csv\", index=False)\n",
        "\n",
        "# # ---------- Transform: resize shortest side to 1024, keep full image ----------\n",
        "# train_transform = transforms.Compose([\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "#     transforms.Resize(1024),    # shortest side = 1024 px, keep aspect ratio\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                          std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# test_transform = transforms.Compose([\n",
        "#     transforms.Resize(1024),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                          std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "\n",
        "\n",
        "# # ---------- Custom Dataset ----------\n",
        "# class SingleLabelDataset(Dataset):\n",
        "#     def __init__(self, dataframe, img_root, transform=None):\n",
        "#         self.df = dataframe.reset_index(drop=True)\n",
        "#         self.root = Path(img_root)\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         row = self.df.iloc[idx]\n",
        "#         img = Image.open(self.root / row.file_name).convert(\"RGB\")\n",
        "#         label = torch.tensor(int(row.grand_id), dtype=torch.long)\n",
        "#         if self.transform:\n",
        "#             img = self.transform(img)\n",
        "#         return img, label\n",
        "\n",
        "# # ---------- Create Datasets & DataLoaders ----------\n",
        "# train_ds = SingleLabelDataset(train_df, images_dir, transform=train_transform)\n",
        "# test_ds  = SingleLabelDataset(test_df,  images_dir, transform=test_transform)\n",
        "\n",
        "# train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4)\n",
        "# test_loader  = DataLoader(test_ds,  batch_size=8, shuffle=False, num_workers=4)\n",
        "\n",
        "# print(\"✓ DataLoaders ready: 1024-pixel images without center crop.\")\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# ---------- Paths ----------\n",
        "data_dir   = Path(\"data\")\n",
        "images_dir = data_dir\n",
        "csv_file   = data_dir / \"image_labels.csv\"\n",
        "\n",
        "# ---------- Train / Test split ----------\n",
        "df = pd.read_csv(csv_file)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
        "print(f\"Train: {len(train_df)}  Test: {len(test_df)}\")\n",
        "\n",
        "# ---------- Transforms ----------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.Resize((1024, 1024)),     # shortest side = 1024, keep aspect ratio\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((1024, 1024)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# ---------- Load all images once and wrap in TensorDataset ----------\n",
        "def make_tensor_dataset(frame, transform):\n",
        "    imgs, labels = [], []\n",
        "    for _, row in frame.iterrows():\n",
        "        img = Image.open(images_dir / row.file_name).convert(\"RGB\")\n",
        "        img = transform(img)\n",
        "        imgs.append(img)\n",
        "        labels.append(int(row.grand_id))\n",
        "    return TensorDataset(torch.stack(imgs),\n",
        "                         torch.tensor(labels, dtype=torch.long))\n",
        "\n",
        "train_ds = make_tensor_dataset(train_df, train_transform)\n",
        "test_ds  = make_tensor_dataset(test_df,  test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=8, shuffle=False)\n",
        "\n",
        "print(\"✓ DataLoaders ready for training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "\tdef __init__(self, in_channels: int, num_classes: int = 10):\n",
        "\t\t\tsuper(CNN, self).__init__()\n",
        "\t\t\t# ---- Convolutional feature extractor ----\n",
        "\t\t\tself.conv1 = nn.Conv2d(in_channels, 8, kernel_size=3, stride=1, padding=1)\n",
        "\t\t\tself.pool  = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\t\t\tself.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "\t\t\t# ---- Global pooling + classifier ----\n",
        "\t\t\t# Always outputs (batch, 16, 1, 1) no matter the input H×W\n",
        "\t\t\tself.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\t\t\tself.fc1 = nn.Linear(16, num_classes)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t\tx = F.relu(self.conv1(x))\n",
        "\t\t\tx = self.pool(x)\n",
        "\t\t\tx = F.relu(self.conv2(x))\n",
        "\t\t\tx = self.gap(x)                # -> (batch, 16, 1, 1)\n",
        "\t\t\tx = torch.flatten(x, 1)        # -> (batch, 16)\n",
        "\t\t\tx = self.fc1(x)                # logits\n",
        "\t\t\treturn x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")   # Use Apple Silicon GPU\n",
        "else:\n",
        "    device = torch.device(\"cpu\")   \n",
        "    \n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grand_id\n",
            "0    618\n",
            "4    188\n",
            "2    145\n",
            "3    125\n",
            "1     45\n",
            "7     35\n",
            "9     25\n",
            "6     15\n",
            "8      3\n",
            "5      1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_df['grand_id'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grand_id\n",
            "0    164\n",
            "4     47\n",
            "3     34\n",
            "2     31\n",
            "7     10\n",
            "1      8\n",
            "9      3\n",
            "6      3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(test_df['grand_id'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "AES3iueY8Bja"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class counts:\n",
            " grand_id\n",
            "0    618\n",
            "1     45\n",
            "2    145\n",
            "3    125\n",
            "4    188\n",
            "5      1\n",
            "6     15\n",
            "7     35\n",
            "8      3\n",
            "9     25\n",
            "Name: count, dtype: int64\n",
            "Class weights:\n",
            " tensor([0.0107, 0.1469, 0.0456, 0.0529, 0.0352, 6.6110, 0.4407, 0.1889, 2.2037,\n",
            "        0.2644], device='mps:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | Train Loss: 2.3355 | Train Acc: 7.67% | Val Acc: 3.67%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 002 | Train Loss: 2.1776 | Train Acc: 9.33% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 003 | Train Loss: 1.7857 | Train Acc: 12.92% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 004 | Train Loss: 1.3701 | Train Acc: 16.00% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 005 | Train Loss: 1.4177 | Train Acc: 13.33% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 006 | Train Loss: 1.2562 | Train Acc: 14.75% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 007 | Train Loss: 1.4000 | Train Acc: 13.00% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 008 | Train Loss: 1.2653 | Train Acc: 13.33% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 009 | Train Loss: 1.1707 | Train Acc: 14.42% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 010 | Train Loss: 1.2314 | Train Acc: 13.50% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 011 | Train Loss: 1.1987 | Train Acc: 13.58% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 012 | Train Loss: 1.1809 | Train Acc: 13.00% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 013 | Train Loss: 1.2171 | Train Acc: 14.25% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 014 | Train Loss: 1.2261 | Train Acc: 13.33% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 015 | Train Loss: 1.1113 | Train Acc: 14.75% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 016 | Train Loss: 1.1370 | Train Acc: 13.58% | Val Acc: 0.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 017 | Train Loss: 1.0058 | Train Acc: 15.92% | Val Acc: 0.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 018 | Train Loss: 1.1207 | Train Acc: 15.33% | Val Acc: 0.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 019 | Train Loss: 1.0774 | Train Acc: 16.50% | Val Acc: 0.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 020 | Train Loss: 1.1698 | Train Acc: 16.25% | Val Acc: 0.67%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 021 | Train Loss: 1.1305 | Train Acc: 16.58% | Val Acc: 0.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 022 | Train Loss: 1.1695 | Train Acc: 18.83% | Val Acc: 0.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 023 | Train Loss: 1.0734 | Train Acc: 21.17% | Val Acc: 0.33%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m running_loss, correct, total = \u001b[32m0.0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m     32\u001b[39m pbar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:212\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/acm_ai/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "# small_train = Subset(train_ds, range(50))\n",
        "# small_loader = DataLoader(small_train, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "model = CNN(in_channels=3, num_classes=10).to(device)\n",
        "\n",
        "class_counts = train_df['grand_id'].value_counts().sort_index()\n",
        "\n",
        "# Inverse-frequency weighting: rarer classes get higher weight\n",
        "weights = 1.0 / class_counts\n",
        "\n",
        "# Normalise so average weight is 1.0 (optional, but keeps loss scale stable)\n",
        "weights = weights / weights.mean()\n",
        "\n",
        "class_weights = torch.tensor(weights.values, dtype=torch.float32).to(device)\n",
        "\n",
        "print(\"Class counts:\\n\", class_counts)\n",
        "print(\"Class weights:\\n\", class_weights)\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) #weight_decay=1e-4\n",
        "\n",
        "epochs = 60\n",
        "for epoch in range(epochs):\n",
        "    # ------------------ TRAIN ------------------\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "\n",
        "    for imgs, labels in pbar:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss   = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total   += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            \"loss\": f\"{running_loss/total:.4f}\",\n",
        "            \"acc\":  f\"{100*correct/total:.2f}%\"\n",
        "        })\n",
        "\n",
        "    train_acc  = 100 * correct / total\n",
        "    train_loss = running_loss / total\n",
        "\n",
        "    # ------------------ VALIDATION ------------------\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            _, preds = outputs.max(1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total   += labels.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Acc: {val_acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "acm_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
